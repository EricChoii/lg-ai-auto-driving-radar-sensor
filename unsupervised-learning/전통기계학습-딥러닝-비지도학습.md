****
### Terms
- KMeans

# 전통기계학습 딥러닝 비지도학습
- Low dimensional data
- Simple concepts
- Reliable?
  - e.g., Clustering cluster validation is a must
    - Internal cluster validation
    - External cluster validation
    - Relative cluster validation

> *Cluster Validation*: 숫자적으로 의미가 있는지 꼭 확인 필요

## [Kmeans Clustering](https://github.com/EricChoii/ai-boot-camp/blob/main/ai/unsupervised-learning/clustering/kmeans.md)
![image](https://user-images.githubusercontent.com/39285147/178944282-5aa8bf0f-77dd-4d93-bd97-21c814c7508d.png)

데이터를 몇 개의 클러스터로 나누어서 비교적 비슷한 특징을 가지는 각각의 클러스터로 모으는 것을 목표

### Other Unsupervised Learning Methods
#### [HCA](https://github.com/EricChoii/ai-boot-camp/blob/main/ai/unsupervised-learning/clustering/hierarchical-clustering.md)

![image](https://user-images.githubusercontent.com/39285147/178954444-58bd1c3c-3aae-453f-93fc-35a73df4c314.png)

#### Density estimation
![image](https://user-images.githubusercontent.com/39285147/178954413-d9a74787-a61f-4115-97b6-7feff2fe42c5.png)

빨갛게 보이는 밑에 있는 선들이 다 각각의 샘플 값들이고, 샘플의 값은 이 경우에 1 dimensional 스칼라 값들이다. 이 때, 어느 스칼라 값 근처에서 점들이 많이 발생하는가에 대한 확률(Density) 계산 문제이다.

#### [PCA](https://github.com/EricChoii/ai-boot-camp/tree/main/ai/unsupervised-learning/pca)
디맨션을 줄이기 위한 기법 Eigenvalue Decomposition과 동일한 방식이다.

## 한계점
전통적 Unsupervised Learning는 실제 애플리케이션에 사용하기에 걱정이 되는 상황.

# Deep Learning (딥러닝에서의 비지도학습)
### Perceptron
![image](https://user-images.githubusercontent.com/39285147/178955645-cb26310a-ba7f-4c1a-9b4b-bb03676b0db5.png)

### Typical Prediction Problem in ML
![image](https://user-images.githubusercontent.com/39285147/178955987-e43adf80-56d8-4000-9e9a-2f7a7a87ca1a.png)

y라는 데이터에 대하여 전처리를 거쳐서 컴퓨터에 들어오는 X를 생성한다.

g() 함수는 지속해서 모델 학습(feature engineering)을 통하여 성장하여 궁극적으로 이상적인 f()함수에 도달한다.

> Feature engineering
>> ML 알고리즘을 작동하기 위해 데이터의 도메인 지식을 활용해 feature를 만드는 과정
>>
>>>> By human
>>>> 
>>>> Domain knowledge & creativity
>>>> 
>>>> Brainstorming

> Representation learning
>> By machine
>> 
>> DL knowledge & coding skill
>> 
>> Trial and error

## Modern Unsupervised Learning
![image](https://user-images.githubusercontent.com/39285147/178944506-0ccd6b97-a4b0-464d-b97c-ccd258221128.png)

- 데이터만 있고, 그에 대한 레이블이 없다.
- High dimensional data
- Difficult concepts
- Not well understood, but surprisingly good performance
- Deep learning
- Unsupervised **Representation Learning*
  - 정확히 어떤 테스크를 풀고 싶은 것인지 정해 놓은 적이 없고, 그저 도움이 되는 정보를 잘 정리해내는 것이 목표이다.

## *Representation*
정보를 어떤 식으로 정리/표현할 것인가?

### Angle 정보
![image](https://user-images.githubusercontent.com/39285147/178958401-115b7981-a6e9-48b4-bc7c-db3bc10bceea.png)

우리가 생각하는 각도라는 개념과 알고리즘이 생각하는 이 숫자에 대한 개념이 상당히 다르므로, Representation을 **어떻게** 하는지가 **성능이 잘 나오는데** 큰 영향을 준다.
- 가령 예시에 나온 문제의 경우, 삼각함수를 사용하여 representation을 수행하면 해결 가능한다.

### 공간 정보
![image](https://user-images.githubusercontent.com/39285147/178960372-0c14ac12-8ae2-4e72-9f44-bde09830cd51.png)

- Human can understand
- Human can design
  - With a goal
  - e.g. Make a specific algorithm work better

> 그래프 이론을 적용하여 효율적인 Representation이 가능하다.

## Representation Learning in Deep Learning
![image](https://user-images.githubusercontent.com/39285147/178961326-c52678ee-a947-4878-874f-09fc0d1025b9.png)

### 한계점
내가 적어놓은 정보는 이렇게 표현해 놓은 것은 충분한가?

## Deep Neural Networks
![image](https://user-images.githubusercontent.com/39285147/178962292-31fa3d88-d089-4a58-b9b7-78cf173de4f5.png)

### 1. Input units
#### *Heavy pre-processing*
Human’s feature engineering
- 데이터가 복잡하여 딥러닝이 충분히 혼자 잘 처리하지 못하는 경우 인간이 많이 도와준다 (새로운 피처를 뽑고 데이터를 잘 정리하여 인풋으로 준다). 그 외 나머지 데이터들은 딥러닝에게 맡긴다.

#### *Minimum pre-processing*
Contrast normalization of images
- 이미지 데이터는 constraints 조정하고 나머지는 모두 딥러닝에게 맡긴다.

#### *No pre-processing*
Text 데이터는 인간에게도 어려운 문제라서 알고리즘에 모든 것을 맡긴다.

### 2. 중간 Layer
Activation vector와 같이 딥러닝이 계산하는 규칙 (we still don't understand how to understand this process)

### 3. Output layers/representation
**Human design factor**
- 가령, 인간이 ReLU와 같은 간단한 Activation를 사용하고 Loss는 Cross Entropy를 사용하는 것이 좋다는 방식과 같은 학습 방법을 직접 제안한다.
- e.g. For classification, use the mathematical object defined as softmax result
  - Form: one hot encoding
  - Prediction value: probability
